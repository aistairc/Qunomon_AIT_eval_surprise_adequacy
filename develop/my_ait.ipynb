{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# AIT Development notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## notebook of structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "| #  | Name                                               | cells | for_dev | edit               | description                                                                |\n",
    "|----|----------------------------------------------------|-------|---------|--------------------|----------------------------------------------------------------------------|\n",
    "| 1  | [Environment detection](##1-Environment-detection) | 1     | No      | uneditable         | detect whether the notebook are invoked for packaging or in production     |\n",
    "| 2  | [Preparing AIT SDK](##2-Preparing-AIT-SDK)         | 1     | Yes     | uneditable         | download and install AIT SDK                                               |\n",
    "| 3  | [Dependency Management](##3-Dependency-Management) | 3     | Yes     | required(cell #2)  | generate requirements.txt for Docker container                             |\n",
    "| 4  | [Importing Libraries](##4-Importing-Libraries)     | 2     | Yes     | required(cell #1)  | import required libraries                                                  |\n",
    "| 5  | [Manifest Generation](##5-Manifest-Generation)     | 1     | Yes     | required           | generate AIT Manifest                                                      |\n",
    "| 6  | [Prepare for the Input](##6-Prepare-for-the-Input) | 1     | Yes     | required           | generate AIT Input JSON (inventory mapper)                                 |\n",
    "| 7  | [Initialization](##7-Initialization)               | 1     | No      | uneditable         | initialization for AIT execution                                           |\n",
    "| 8  | [Function definitions](##8-Function-definitions)   | N     | No      | required           | define functions invoked from Main area.<br> also define output functions. |\n",
    "| 9  | [Main Algorithms](##9-Main-Algorithms)             | 1     | No      | required           | area for main algorithms of an AIT                                         |\n",
    "| 10 | [Entry point](##10-Entry-point)                    | 1     | No      | uneditable         | an entry point where Qunomon invoke this AIT from here                     |\n",
    "| 11 | [License](##11-License)                            | 1     | Yes     | required           | generate license information                                               |\n",
    "| 12 | [Deployment](##12-Deployment)                      | 1     | Yes     | uneditable         | convert this notebook to the python file for packaging purpose             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## notebook template revision history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "1.0.1 2020/10/21\n",
    "\n",
    "* add revision history\n",
    "* separate `create requirements and pip install` editable and noeditable\n",
    "* separate `import` editable and noeditable\n",
    "\n",
    "1.0.0 2020/10/12\n",
    "\n",
    "* new cerarion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### #1 Environment detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "[uneditable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Determine whether to start AIT or jupyter by startup argument\n",
    "import sys\n",
    "is_ait_launch = (len(sys.argv) == 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### #2 Preparing AIT SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "[uneditable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "if not is_ait_launch:\n",
    "    # get ait-sdk file name\n",
    "    from pathlib import Path\n",
    "    from glob import glob\n",
    "    import re\n",
    "    import os\n",
    "\n",
    "    current_dir = %pwd\n",
    "\n",
    "    ait_sdk_path = \"./ait_sdk-*-py3-none-any.whl\"\n",
    "    ait_sdk_list = glob(ait_sdk_path)\n",
    "    ait_sdk_name = os.path.basename(ait_sdk_list[-1])\n",
    "\n",
    "    # install ait-sdk\n",
    "    !pip install -q --upgrade pip\n",
    "    !pip install -q --no-deps --force-reinstall ./$ait_sdk_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### #3 Dependency Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### #3-1 [uneditable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "if not is_ait_launch:\n",
    "    from ait_sdk.common.files.ait_requirements_generator import AITRequirementsGenerator\n",
    "    requirements_generator = AITRequirementsGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### #3-2 [required]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_ait_launch:\n",
    "    requirements_generator.add_package('numpy', '1.26.3')\n",
    "    requirements_generator.add_package('matplotlib', '3.7.3')\n",
    "    requirements_generator.add_package('pandas', '2.2.2')\n",
    "    requirements_generator.add_package('scikit-learn', '1.4.2')\n",
    "    requirements_generator.add_package('tensorflow', '2.11.1')\n",
    "    requirements_generator.add_package('tqdm', '4.66.2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### #3-3 [uneditable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "if not is_ait_launch:\n",
    "    requirements_generator.add_package(f'./{ait_sdk_name}')\n",
    "    requirements_path = requirements_generator.create_requirements(current_dir)\n",
    "\n",
    "    !pip install -q -r $requirements_path "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### #4 Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### #4-1 [required]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-18 14:38:17.569944: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-18 14:38:17.671970: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-04-18 14:38:17.671990: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-04-18 14:38:18.356493: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-04-18 14:38:18.356592: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-04-18 14:38:18.356598: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Input, Flatten, Dense, Lambda, Reshape, BatchNormalization, MaxPooling2D, Dropout\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "from collections import defaultdict \n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from scipy import stats\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score \n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### #4-2 [uneditable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# must use modules\n",
    "from os import path\n",
    "import shutil  # do not remove\n",
    "from ait_sdk.common.files.ait_input import AITInput  # do not remove\n",
    "from ait_sdk.common.files.ait_output import AITOutput  # do not remove\n",
    "from ait_sdk.common.files.ait_manifest import AITManifest  # do not remove\n",
    "from ait_sdk.develop.ait_path_helper import AITPathHelper  # do not remove\n",
    "from ait_sdk.utils.logging import get_logger, log, get_log_path  # do not remove\n",
    "from ait_sdk.develop.annotation import measures, resources, downloads, ait_main  # do not remove\n",
    "# must use modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### #5 Manifest Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "[required]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_ait_launch:\n",
    "    from ait_sdk.common.files.ait_manifest_generator import AITManifestGenerator\n",
    "    manifest_generator = AITManifestGenerator(current_dir)\n",
    "    manifest_generator.set_ait_name('eval_surprise_adequacy')\n",
    "    manifest_generator.set_ait_description('入力VAEモデルのサプライズ適切性（SA）を計算しています。\\\n",
    "SAは、入力データの各サンプルに対する各ニューロンの活動トレースを評価します。\\\n",
    "詳細については、元の論文「Kim, et al. Evaluating Surprise Adequacy for Deep Learning System Testing」（URL: https://dl.acm.org/doi/full/10.1145/3546947）')\n",
    "    manifest_generator.set_ait_source_repository('https://github.com/aistairc/Qunomon_AIT_eval_surprise_adequacy')\n",
    "    manifest_generator.set_ait_version('1.0')\n",
    "    manifest_generator.add_ait_keywords('evaluation')\n",
    "    manifest_generator.set_ait_quality('https://ait-hub.pj.aist.go.jp/ait-hub/api/0.0.1/qualityDimensions/機械学習品質マネジメントガイドライン第三版/A-1問題領域分析の十分性')\n",
    "    \n",
    "    inventory_requirement_cifar100_data      = manifest_generator.format_ait_inventory_requirement(format_=['npz'])\n",
    "    inventory_requirement_cifar10_data       = manifest_generator.format_ait_inventory_requirement(format_=['npz'])\n",
    "    inventory_requirement_fashion_mnist_data = manifest_generator.format_ait_inventory_requirement(format_=['npz'])\n",
    "    inventory_requirement_mnist_data         = manifest_generator.format_ait_inventory_requirement(format_=['npz'])\n",
    "\n",
    "    manifest_generator.add_ait_inventories(name='mnist_data', type_='dataset', description='mnist data', requirement=inventory_requirement_mnist_data)\n",
    "    manifest_generator.add_ait_inventories(name='fashion_mnist_data', type_='dataset', description='fashion mnist data', requirement=inventory_requirement_fashion_mnist_data)\n",
    "    manifest_generator.add_ait_inventories(name='cifar10_data', type_='dataset', description='cifar10 data', requirement=inventory_requirement_cifar10_data)\n",
    "    manifest_generator.add_ait_inventories(name='cifar100_data', type_='dataset', description='cifar100 data', requirement=inventory_requirement_cifar100_data)\n",
    "    \n",
    "    inventory_requirement_vae_model = manifest_generator.format_ait_inventory_requirement(format_=['h5'])\n",
    "    manifest_generator.add_ait_inventories(name='vae', type_='model', description='learned model', requirement=inventory_requirement_vae_model)\n",
    "\n",
    "    ### input parameters, Hyperparameters\n",
    "    manifest_generator.add_ait_parameters(name='latent_dim', type_='int', default_val='100', description='Hyperparameter specifying the latent space dimension')\n",
    "    manifest_generator.add_ait_parameters(name='batch_size', type_='int', default_val='32', description='Hyperparameter specifying the batch size of the optimizer of VAE')\n",
    "\n",
    "    ### input parameters\n",
    "    manifest_generator.add_ait_parameters(name='datasetName', type_='str', default_val='mnist', description='Parameter specifying dataset')\n",
    "    manifest_generator.add_ait_parameters(name='noise_perc', type_='float', default_val='20', description='Parameter specifying the percentage of noised labels')\n",
    "    manifest_generator.add_ait_parameters(name='noise_systematic', type_='str', default_val='Sys', description='Parameter specifying the type to add noise according to the label values (Sys) or random (Uni)')\n",
    "    manifest_generator.add_ait_parameters(name='model_name', type_='str', default_val='', description='Parameter specifying VAE model')\n",
    "\n",
    "    ### output\n",
    "    manifest_generator.add_ait_downloads(name='DSA', description='DSA of given data with given model')\n",
    "    manifest_generator.add_ait_downloads(name='Log', description='AIT実行ログ')\n",
    "    manifest_path = manifest_generator.write()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### #6 Prepare for the Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "[required]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_ait_launch:\n",
    "    from ait_sdk.common.files.ait_input_generator import AITInputGenerator\n",
    "    input_generator = AITInputGenerator(manifest_path)\n",
    "    input_generator.add_ait_inventories(name='mnist_data',\n",
    "                                        value='mnist_data/mnist_train_data.npz')\n",
    "    input_generator.add_ait_inventories(name='fashion_mnist_data',\n",
    "                                        value='fashion_mnist_data/fashion_mnist_train_data.npz')\n",
    "    input_generator.add_ait_inventories(name='cifar10_data',\n",
    "                                        value='cifar10_data/cifar10_train_data.npz')\n",
    "    input_generator.add_ait_inventories(name='cifar100_data',\n",
    "                                        value='cifar100_data/cifar100_train_data.npz')\n",
    "\n",
    "    \n",
    "    ### hyperparameter\n",
    "    latent_dim = 100\n",
    "    batch_size = 32\n",
    "    input_generator.set_ait_params(name='latent_dim', value=latent_dim)\n",
    "    input_generator.set_ait_params(name='batch_size', value=batch_size)\n",
    "    \n",
    "    ### input parameters\n",
    "    datasetName = 'mnist'\n",
    "    noise_perc = 10\n",
    "    noise_systematic = 'Sys'\n",
    "    model_name = f'vae_{datasetName}_{noise_systematic}_{noise_perc}.keras'\n",
    "\n",
    "    input_generator.set_ait_params(name='datasetName', value=datasetName)\n",
    "    input_generator.set_ait_params(name='noise_perc', value=noise_perc)\n",
    "    input_generator.set_ait_params(name='noise_systematic', value=noise_systematic)\n",
    "    input_generator.set_ait_params(name='model_name', value=model_name)\n",
    "    input_generator.add_ait_inventories(name='vae', value=f'vae_model/{model_name}')\n",
    "    \n",
    "    input_generator.write()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### #7 Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "[uneditable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "logger = get_logger()\n",
    "\n",
    "ait_manifest = AITManifest()\n",
    "ait_input = AITInput(ait_manifest)\n",
    "ait_output = AITOutput(ait_manifest)\n",
    "\n",
    "if is_ait_launch:\n",
    "    # launch from AIT\n",
    "    current_dir = path.dirname(path.abspath(__file__))\n",
    "    path_helper = AITPathHelper(argv=sys.argv, ait_input=ait_input, ait_manifest=ait_manifest, entry_point_dir=current_dir)\n",
    "else:\n",
    "    # launch from jupyter notebook\n",
    "    # ait.input.json make in input_dir\n",
    "    input_dir = '/usr/local/qai/mnt/ip/job_args/1/1'\n",
    "    current_dir = %pwd\n",
    "    path_helper = AITPathHelper(argv=['', input_dir], ait_input=ait_input, ait_manifest=ait_manifest, entry_point_dir=current_dir)\n",
    "\n",
    "ait_input.read_json(path_helper.get_input_file_path())\n",
    "ait_manifest.read_json(path_helper.get_manifest_file_path())\n",
    "\n",
    "### do not edit cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### #8 Function definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "[required]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name: str = 'vae_model'):\n",
    "    vae_model = keras.models.load_model(model_name, compile=False)\n",
    "    return vae_model\n",
    "\n",
    "@log(logger)\n",
    "@downloads(ait_output, path_helper, 'DSA', 'DSA_values.csv')\n",
    "def calculate_DSA(vae_model, train_data, file_path: str = None):\n",
    "    encoder = vae_model.get_layer('Encoder')\n",
    "    decoder = vae_model.get_layer('Decoder')\n",
    "    classifier = vae_model.get_layer('Classifier')\n",
    "\n",
    "    print('Encoder prediction ...')\n",
    "    train_activations = encoder.predict(train_data)[2]\n",
    "    print('Classifier prediction ...')\n",
    "    train_predictions = classifier.predict(train_activations)\n",
    "    train_predictions = np.argmax(train_predictions, axis=1)\n",
    "    valid_indices = np.logical_and(np.isfinite(train_activations).all(axis=1),\n",
    "                                   np.isfinite(train_predictions))\n",
    "    train_activations = train_activations[valid_indices]\n",
    "    train_predictions = train_predictions[valid_indices]\n",
    "\n",
    "    def _DSA(X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        dsa_scores = np.zeros(n_samples)\n",
    "\n",
    "        for i in tqdm(range(n_samples), desc='Calculating DSA'):\n",
    "            distances = np.linalg.norm(X - X[i], axis=1)\n",
    "            same_class_mask = (y == y[i])\n",
    "            diff_class_mask = (y != y[i])\n",
    "\n",
    "            same_class_distances = distances[same_class_mask]\n",
    "            diff_class_distances = distances[diff_class_mask]\n",
    "\n",
    "            min_same_class_distance = np.min(same_class_distances[same_class_distances > 0])\n",
    "            min_diff_class_distance = np.min(diff_class_distances)\n",
    "\n",
    "            dsa_scores[i] = min_same_class_distance / min_diff_class_distance\n",
    "\n",
    "        return dsa_scores\n",
    "\n",
    "    surprise_adequacy_dsa = _DSA(train_activations, train_predictions)\n",
    "    pd.DataFrame(surprise_adequacy_dsa).to_csv(file_path)\n",
    "    return surprise_adequacy_dsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log(logger)\n",
    "@downloads(ait_output, path_helper, 'Log', 'ait.log')\n",
    "def move_log(file_path: str=None) -> str:\n",
    "    shutil.move(get_log_path(), file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### #9 Main Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "[required]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@log(logger)\n",
    "@ait_main(ait_output, path_helper)\n",
    "def main() -> None:\n",
    "    input_data = np.load(ait_input.get_inventory_path('mnist_data'))\n",
    "    model = load_model(ait_input.get_inventory_path('vae'))\n",
    "    dsa = calculate_DSA(model, input_data['X'])\n",
    "    move_log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### #10 Entry point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "[uneditable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-18 14:38:20.562953: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2024-04-18 14:38:20.563002: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2024-04-18 14:38:20.563022: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (1bb34e1998b9): /proc/driver/nvidia/version does not exist\n",
      "2024-04-18 14:38:20.563219: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder prediction ...\n",
      "1875/1875 [==============================] - 12s 6ms/step\n",
      "Classifier prediction ...\n",
      "1875/1875 [==============================] - 3s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating DSA:   0%|          | 0/57163 [00:00<?, ?it/s]/usr/local/lib/python3.9/site-packages/numpy/linalg/linalg.py:2582: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n",
      "/tmp/ipykernel_29/76633088.py:37: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  dsa_scores[i] = min_same_class_distance / min_diff_class_distance\n",
      "Calculating DSA:   0%|          | 211/57163 [00:01<08:26, 112.48it/s]/usr/local/lib/python3.9/site-packages/numpy/linalg/linalg.py:2583: RuntimeWarning: overflow encountered in reduce\n",
      "  return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n",
      "Calculating DSA:   2%|▏         | 1324/57163 [00:12<08:20, 111.53it/s]/tmp/ipykernel_29/76633088.py:27: RuntimeWarning: overflow encountered in subtract\n",
      "  distances = np.linalg.norm(X - X[i], axis=1)\n",
      "Calculating DSA: 100%|██████████| 57163/57163 [09:16<00:00, 102.79it/s]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### #11 License"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "[required]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ait_owner='AIST'\n",
    "ait_creation_year='2024'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### #12 Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "[uneditable] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "if not is_ait_launch:\n",
    "    from ait_sdk.deploy import prepare_deploy\n",
    "    from ait_sdk.license.license_generator import LicenseGenerator\n",
    "    \n",
    "    current_dir = %pwd\n",
    "    prepare_deploy(ait_sdk_name, current_dir, requirements_path)\n",
    "    \n",
    "    # output License.txt\n",
    "    license_generator = LicenseGenerator()\n",
    "    license_generator.write('../top_dir/LICENSE.txt', ait_creation_year, ait_owner)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "cc00c6a56d87bd8bd7773e730c60ddfdb8804da6b7537df09499efbcf81630f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
